# Лабораторная работа 1 «Метод обратного распространения ошибки»

## Цели работы

**Цель** — изучить метод обратного распространения ошибки для обучения глубоких нейронных сетей на примере двухслойной полностью связанной сети (один скрытый слой).

## Задачи работы

Выполнение работы предполагает решение следующих задач:

1. Изучение общей схемы метода обратного распространения ошибки на базе стохастического градиентного спуска.
1. Вывод математических формул для вычисления градиентов функции ошибки по параметрам нейронной сети и формул коррекции весов.
1. Проектирование и разработка программной реализации метода обратного распространения ошибки.
1. Тестирование разработанной программной реализации.
1. Разработка примера использования метода обратного распространения ошибки для классификации рукописных цифр из набора данных [MNIST](https://www.kaggle.com/datasets/hojjatk/mnist-dataset).

Метод обратного распространения ошибки разрабатывается, исходя из следующих предположений:

1. На входе сети имеется `w x h` нейронов, что соответствует разрешению одноканального изображения (разрешение — параметр; для изображений в базе MNIST составляет 28x28).
1. На выходе сети имеется `k` нейронов, что соответствует количеству классов изображений (количество выходных нейронов — параметр; для задачи классификации рукописных цифр – 10 классов).
1. Скрытый слой содержит `s` нейронов (параметр).
1. Скорость обучения (learning rate), размер пачки данных (batch size), количество эпох являются параметрами метода обучения (параметры).
1. В качестве функции активации на скрытом слое используется функция ReLU.
1. В качестве функции активации на выходном слое используется функция softmax.
1. В качестве функции ошибки используется кросс-энтропия.

## Требования к результатам выполнения работы

1. Разработана корректная программная реализация метода обратного распространения ошибки для рассматриваемой архитектуры нейронной сети и приложение для решения задачи классификации рукописных цифр на примере базы MNIST на языке Python 3 (в Jupiter Notebook). В результате сформирован скрипт в формате `.ipynb` и преобразованный Jupiter Notebook в формат `.html` с выдачей промежуточных результатов на контрольном наборе параметров (приведен ниже).

1. Требования к структуре и содержимому Jupiter Notebook:

   - Проведена проверка корректности загрузки данных (совпадение размерностей, визуальное отображение выборочных данных).
   - По окончании каждой эпохи в процессе обучения модели выводится ошибка классификации на тренировочном наборе данных.
   - По окончании эпохи в процессе обучения выводится время эпохи.
   - После обучения выводится ошибка классификации на тестовом наборе данных.

1. Для контрольных значений параметров достигнута точность классификации на тестовых данных, сравнимая с точностью, которую выдают стандартные инструменты глубокого обучения (например, библиотеки MXNet или PyTorch). Подсказка: Точность будет ~95% без правильной инициализации весов, ожидается результат лучше.

1. Запуск скрипта выполнен в процессе очной сдачи, и время обучения на контрольном наборе параметров не должно превышать время, выделенное на сдачу работу одним студентом (~7-10 минут).

**Контрольный набор параметров:**

- Размер пачки может меняться от 8 до 64 изображений (в зависимости от доступного объема памяти).
- Скорость обучения - 0.1.
- Количество скрытых нейронов – 300.
- Количество эпох – 20.
